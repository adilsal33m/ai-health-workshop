{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9503057,
          "sourceType": "datasetVersion",
          "datasetId": 5783657
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "AI Health Workshop - Summarization",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† The Information Overload Challenge in Healthcare\n",
        "\n",
        "Modern healthcare professionals are faced with an enormous amount of patient data ‚Äî from electronic health records (EHRs) and clinical notes to lab results and imaging reports. Processing all this data efficiently is often not feasible in time-critical scenarios.\n",
        "\n",
        "### üîé Illustrative Example\n",
        "\n",
        "Let‚Äôs consider a doctor reviewing a patient's EHR before making a treatment decision. The EHR might include:\n",
        "\n",
        "- üßæ 3‚Äì5 pages of discharge summaries  \n",
        "- üí¨ Progress notes from multiple specialists  \n",
        "- üß™ Lab results over several weeks  \n",
        "- üìã Medication history  \n",
        "- üñºÔ∏è Radiology reports  \n",
        "\n",
        "Now, imagine this real-world snippet from a progress note:\n",
        "\n",
        "> *The patient has been having arm pain for several months. She underwent an exercise stress echocardiogram within the last several months with me, which was equivocal, but then she had a nuclear stress test which showed inferobasilar ischemia. I had originally advised her for a heart catheterization but she wanted medical therapy, so we put her on a beta-blocker. However, her arm pain symptoms accelerated and she had some jaw pain, so she presented to the emergency room. On 08/16/08, she ended up having a cardiac catheterization and that showed normal left main 80% mid LAD lesion, circumflex normal, and RCA totally occluded in the mid portion and there were collaterals from the left to the right, as well as right to right to that area. The decision was made to transfer her as she may be having collateral insufficiency from the LAD stenosis to the RCA vessel. She underwent that with drug-eluting stents on 08/16/08, with I believe three or four total placed, and was discharged on 08/17/08. She had some left arm discomfort on 08/18/08, but this was mild. Yesterday, she felt very fatigued, but no arm pain, and today, she had arm pain after walking and again it resolved now completely after three sublingual nitroglycerin. This is her usual angina. She is being admitted with unstable angina post stent.*\n",
        "\n",
        "Even this short note contains **critical information** hidden within lots of data which may not be as important. Multiply that by dozens of such notes per patient per visit, and you get a sense of the cognitive burden.\n",
        "\n",
        "‚û°Ô∏è **This is where summarization tools can play a critical role ‚Äî by distilling the most important clinical information quickly and accurately.**\n",
        "\n",
        "Consider this summary:\n",
        "\n",
        "> *The patient, experiencing arm pain for several months, initially declined heart catheterization and was treated with a beta-blocker following equivocal and ischemia-indicating stress tests. Her symptoms worsened with jaw pain, prompting an ER visit. On 08/16/08, cardiac catheterization revealed significant LAD stenosis and complete RCA occlusion with collateral circulation. She received multiple drug-eluting stents and was discharged on 08/17/08. Subsequent mild symptoms included arm discomfort and fatigue. She is now being admitted with unstable angina post-stenting.*"
      ],
      "metadata": {
        "_uuid": "5d832a92-e363-42a9-9f4b-d1c8f8a250b1",
        "_cell_guid": "22cc5cf5-6502-4ad6-b1c5-4ba74a0c6820",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "w7m07VItl9OA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß≠ Types of Summarization:\n",
        "\n",
        "When it comes to automatic text summarization, there are two primary approaches:\n",
        "\n",
        "- Extractive Summarization\n",
        "- Abstractive Summarization\n",
        "\n",
        "### üîç Key Differences\n",
        "\n",
        "| Feature                  | Extractive                         | Abstractive                        |\n",
        "|--------------------------|-------------------------------------|------------------------------------|\n",
        "| Output Style             | Copies original text                | Generates new text                 |\n",
        "| Coherence                | Sometimes fragmented                | Generally more fluent              |\n",
        "| Factual Reliability      | High (uses original sentences)      | Moderate (may hallucinate facts)   |\n",
        "| Interpretability         | Easy to trace back                  | Harder to trace                    |\n",
        "| Computational Demand     | Lower                               | Higher                             |\n",
        "| Domain Adaptability      | Moderate                            | High (with fine-tuning)            |\n",
        "\n",
        "---\n",
        "\n",
        "‚û°Ô∏è In healthcare settings, **extractive methods** are useful for quick highlighting, while **abstractive summarization** can offer deeper understanding and better readability ‚Äî especially for clinical notes, discharge summaries, and radiology reports."
      ],
      "metadata": {
        "_uuid": "220f085a-836a-47d6-bbc2-4048202ef665",
        "_cell_guid": "d9ffb15d-0211-4f9e-8164-091de06a6047",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "cGPcMz7fl9OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h1 style=\"color:red;\">Before you start</h1>\n",
        "\n",
        "## Enable the accelerator\n",
        "Platform like Kaggle and Google Colab are useful because they allow us to use GPU (accelerator) for computation which makes running most of the load-intensive models faster. But before we can do that, we need to enable the accelerator. To do so, follow these steps:\n",
        "\n",
        "1. Press the \"Runtime\" button on the top bar (just below the notebook title)\n",
        "2. Select \"Change runtime type\"\n",
        "3. Select either \"T4 GPU\"\n",
        "4. Done.\n"
      ],
      "metadata": {
        "id": "VSpiq35Zpsvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Steps: Loading a dataset"
      ],
      "metadata": {
        "_uuid": "b7e712d9-c168-4e17-9db2-1bc6ed9263a1",
        "_cell_guid": "f4bbcfbc-ee1b-4d91-b834-b0e2a6720a3e",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-ySNFHQyl9OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "_uuid": "1363763b-cc42-4618-a5d1-a7620d844231",
        "_cell_guid": "5a9efed1-c8da-4ed0-9c45-71f4ec356758",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "oNWxQLpDl9OG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/singla007/MTSamples/refs/heads/main/mtsamples.csv',index_col=None)\n",
        "df.drop_duplicates(subset=['description'],inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "_uuid": "f01f62c4-663f-46ce-b93c-8367124a1358",
        "_cell_guid": "e90fceb4-49f8-437d-9dcb-91cca8cad426",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "dZZbRAHnl9OG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "_uuid": "e4759ad9-c404-4e85-9097-7fc77e70de9e",
        "_cell_guid": "7a2f2f95-4f77-4a02-85d5-d89560ff90af",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GQAcryoxl9OG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Extractive Summarization\n",
        "\n",
        "**Definition:**  \n",
        "Extractive summarization involves selecting and concatenating the most important sentences or phrases directly from the original text ‚Äî without altering the wording.\n",
        "\n",
        "**Strengths:**\n",
        "- Fast and computationally efficient.\n",
        "- Maintains factual correctness (since it uses original sentences).\n",
        "- Useful when interpretability and traceability are important.\n",
        "\n",
        "**Weaknesses:**\n",
        "- May result in redundancy or lack of coherence.\n",
        "- Often lacks the ability to paraphrase or synthesize ideas.\n",
        "- Does not reduce verbosity as effectively as abstractive methods.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "_uuid": "071be467-9685-4c73-83e1-8a8c1e3a7819",
        "_cell_guid": "716d6d0d-1c37-456e-bc4c-36a28b048a5b",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "U95fIcw5l9OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1: Extractive summary using TF-IDF\n",
        "\n",
        "### üß† Intuition Behind Extractive Summarization using TF-IDF\n",
        "\n",
        "Extractive summarization with **TF-IDF (Term Frequency-Inverse Document Frequency)** is based on a simple idea:\n",
        "\n",
        "> üó£Ô∏è **\"Sentences that contain rare but important words (i.e., informative terms) are likely to be good summary candidates.\"**\n",
        "\n",
        "### ‚úçÔ∏è How It Works\n",
        "\n",
        "1. **Split the text into sentences**.\n",
        "2. **Convert each sentence and the entire document into TF-IDF vectors**:\n",
        "   - Words that appear frequently in a sentence but not frequently across all sentences get **higher scores**.\n",
        "3. **Compute cosine similarity** between each sentence vector and the full document vector:\n",
        "   - This tells us how *representative* a sentence is of the entire document.\n",
        "4. **Pick the top N most similar sentences** as the summary.\n",
        "\n",
        "---\n",
        "\n",
        "### üñºÔ∏è Visualization\n",
        "\n",
        "Below is a graphic that captures the core idea of TF-IDF-based extractive summarization:\n",
        "\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/tf-idf.png?raw=true\" width=400 style=\"margin: 0 auto;\"/>\n",
        "\n",
        "*Illustration: Sentences are ranked by their similarity to the full document. The most representative ones are selected.*\n",
        "\n",
        "---\n",
        "\n",
        "In clinical narratives or medical records:\n",
        "- Keywords like _\"ischemia\"_, _\"occlusion\"_, or _\"stent\"_ carry critical meaning.\n",
        "- TF-IDF helps us **surface these high-information sentences** without needing deep learning models.\n",
        "- It‚Äôs **fast**, **transparent**, and **interpretable**."
      ],
      "metadata": {
        "_uuid": "3a22e0b0-1d68-45b1-8501-2fe97cb0a48c",
        "_cell_guid": "8af27a2a-0310-4ca3-a326-fe5c48c8a7f8",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "led1VKyXl9OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "text = df.iloc[idx]['transcription']\n",
        "print(text)"
      ],
      "metadata": {
        "_uuid": "66b41d8d-51b3-4188-98df-a15829ff0761",
        "_cell_guid": "f848bd28-1951-4779-9fe7-56481f19cedb",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "VgOX20a3l9OH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "5Wzt0VAQnGf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "\n",
        "# Step 1: Sentence Tokenization\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# Step 2: TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Step 3: Compute cosine similarity of each sentence with the whole text\n",
        "document_vector = vectorizer.transform([text])\n",
        "similarity_scores = cosine_similarity(tfidf_matrix, document_vector).flatten()\n",
        "\n",
        "# Step 4: Select top-N most similar sentences\n",
        "top_n = 3\n",
        "top_indices = heapq.nlargest(top_n, range(len(similarity_scores)), similarity_scores.__getitem__)\n",
        "summary = [sentences[i] for i in sorted(top_indices)]  # sort to maintain original order\n",
        "\n",
        "# Display extractive summary\n",
        "print(\"üìå Extractive Summary:\\n\")\n",
        "for sent in summary:\n",
        "    pprint(sent.strip())"
      ],
      "metadata": {
        "_uuid": "2abb413b-3c2a-421c-879c-67beb429991e",
        "_cell_guid": "c6516b44-4c23-4ff4-9951-7bf2c16a5096",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "sQmKLvlql9OH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: Extractive Summarization using TextRank\n",
        "\n",
        "### üß† Intuition Behind Extractive Summarization using TextRank\n",
        "\n",
        "TextRank is a **graph-based ranking algorithm**, inspired by Google's PageRank. Instead of ranking web pages, it ranks **sentences** based on how well they are connected (semantically similar) to each other.\n",
        "\n",
        "> üó£Ô∏è **\"Important sentences are those that are most similar to other important sentences.\"**\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ How It Works\n",
        "\n",
        "1. **Split the text into sentences.**\n",
        "2. **Compute pairwise similarity** between all sentences using cosine similarity (based on word embeddings, TF-IDF, or other metrics).\n",
        "3. **Construct a graph** where:\n",
        "   - **Nodes = sentences**\n",
        "   - **Edges = similarity scores** between sentences\n",
        "4. **Apply the TextRank algorithm**:\n",
        "   - Sentences that are similar to many other sentences get higher ranks.\n",
        "5. **Select the top-ranked sentences** as the summary.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üñºÔ∏è Visualization\n",
        "\n",
        "Below is a diagram showing how TextRank works from raw text to extractive summary:\n",
        "\n",
        "![text_rank_flowchart.png](https://github.com/adilsal33m/ai-health-workshop/blob/main/text_rank_flowchart.png?raw=true)\n",
        "\n",
        "*Illustration: Sentences are ranked by centrality in a similarity graph. Top-ranked ones are extracted as summary.*\n",
        "---\n",
        "\n",
        "### ‚úÖ Why It Works (Especially in Healthcare)\n",
        "\n",
        "- Captures **global relevance** by comparing sentences with each other.\n",
        "- Doesn‚Äôt require training data ‚Äî unsupervised and interpretable.\n",
        "- In healthcare, it can **highlight common clinical themes** or repeated key findings."
      ],
      "metadata": {
        "_uuid": "b3eb5da3-6d77-469e-9bef-f870d6cab8d5",
        "_cell_guid": "6ead5eec-f612-47cc-9b7e-c4c5123e6617",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JBHBEIlpl9OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytextrank --quiet"
      ],
      "metadata": {
        "_uuid": "aab4fbcd-e574-45e1-8d0b-08e971a5ec8c",
        "_cell_guid": "fbc05ec3-e7b6-4a16-9a6b-3db956e98e72",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "vFod515Fl9OI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 6\n",
        "text = df.iloc[idx]['transcription']\n",
        "print(text)"
      ],
      "metadata": {
        "_uuid": "40087aed-6b78-4cc9-822c-fdb2949cbc67",
        "_cell_guid": "d8ae58a7-71df-4e22-b8af-6bc630d1d8b0",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "yNnMa-_Bl9OI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pytextrank\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"textrank\")\n",
        "\n",
        "doc = nlp(text)\n",
        "for sent in doc._.textrank.summary(limit_sentences=4):\n",
        "    print(\"- \",sent,end=\"\\n\\n\")"
      ],
      "metadata": {
        "_uuid": "f992a054-f838-44d4-afcf-c3e25ed63950",
        "_cell_guid": "b975b677-9cee-4a80-a659-330742ab681a",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "B3igpkG2l9OI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 3: Extractive Summarization using Sumy\n",
        "\n",
        "### üß† Intuition Behind Extractive Summarization using `Sumy`\n",
        "\n",
        "In real-world research and prototyping, it's often inefficient to implement every algorithm from scratch. This is where libraries like **[Sumy](https://github.com/miso-belica/sumy)** come into play ‚Äî offering **well-tested and optimized implementations** of popular extractive summarization techniques.\n",
        "\n",
        "> üó£Ô∏è **\"Sumy lets you focus on *what to summarize* rather than *how to build a summarizer from the ground up*.\"**\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ What is Sumy?\n",
        "\n",
        "`Sumy` is a lightweight Python library that provides **off-the-shelf extractive summarization algorithms**, including:\n",
        "\n",
        "- **TextRank**\n",
        "- **LexRank**\n",
        "- **Luhn**\n",
        "- **LSA (Latent Semantic Analysis)**\n",
        "- **Edmundson Method**\n",
        "\n",
        "All with minimal setup and clean interfaces.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How Sumy Works (Under the Hood)\n",
        "\n",
        "1. **Input**: A text document or paragraph.\n",
        "2. **Preprocessing**: Tokenization and normalization.\n",
        "3. **Sentence Scoring**: Uses the chosen algorithm (e.g., TextRank or LexRank) to rank sentences.\n",
        "4. **Selection**: Extracts top-ranked sentences to generate the summary.\n",
        "\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/sumy.png?raw=true\" width=200 style=\"margin: 0 auto;\"/>\n",
        "\n",
        "You don‚Äôt have to manage:\n",
        "- Sentence embeddings  \n",
        "- Graph construction  \n",
        "- Matrix factorization  \n",
        "- TF-IDF computations  \n",
        "\n",
        "Sumy abstracts these details into a few function calls.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why Researchers Use Sumy\n",
        "\n",
        "- üî¨ **Rapid Prototyping**: Perfect for evaluating ideas quickly in NLP workflows.\n",
        "- üì¶ **Reliable Algorithms**: Implements canonical versions of classic summarizers.\n",
        "- üí¨ **Human-Readable Summaries**: Extracts coherent, well-formed sentences.\n",
        "- üß™ **Useful in Domains like Healthcare**: When paired with preprocessing, Sumy can summarize long clinical notes efficiently."
      ],
      "metadata": {
        "_uuid": "c9678723-3aac-47fb-bb40-24a2cc6bfb12",
        "_cell_guid": "6a8c5aa9-ad87-4dd4-86e1-116ead1274ef",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "gQYAvAxJl9OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy --quiet"
      ],
      "metadata": {
        "_uuid": "f0e1773f-51a8-406c-acb2-dbd9149a9b5b",
        "_cell_guid": "d613d559-1222-4dad-96d1-cd0811912c27",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Xl5NXcYBl9OI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 1\n",
        "text = df.iloc[idx]['transcription']\n",
        "print(text)"
      ],
      "metadata": {
        "_uuid": "d455ed84-0e68-4582-af53-c5338d1100b6",
        "_cell_guid": "29f76978-b8a1-4c42-b95b-c8e82a5f92b9",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "iLd1xmJXl9OJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer = LsaSummarizer()\n",
        "summary = summarizer(parser.document, 3)\n",
        "for sentence in summary:\n",
        "    print(\"-\",sentence,end=\"\\n\\n\")"
      ],
      "metadata": {
        "_uuid": "07e2edaf-98ba-4e4a-9fbc-2957239d3f45",
        "_cell_guid": "135a24d9-dafd-4f3c-bd24-4259d31f5f3e",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "mXMPZmKml9OJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 4: BERT based extractive summarizer\n",
        "\n",
        "### üß† Intuition Behind BERT-Based Extractive Summarization\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a powerful language model trained to understand the **context of words in a sentence**. When applied to extractive summarization, BERT helps in selecting the most **semantically relevant** sentences from a document.\n",
        "\n",
        "> üó£Ô∏è **\"Instead of relying on simple word frequency or graph-based similarity, BERT understands the meaning of sentences in context and learns to choose the most informative ones.\"**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úçÔ∏è How It Works\n",
        "\n",
        "1. **Split the document into sentences.**\n",
        "2. **Encode each sentence using BERT**, which converts them into high-dimensional embeddings capturing their semantics.\n",
        "3. **Model sentence importance** using either:\n",
        "   - A scoring mechanism (like cosine similarity with a document vector), or\n",
        "   - A classifier trained to predict whether a sentence should be part of the summary.\n",
        "4. **Rank and select the top sentences** based on their scores.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why It Works (Especially in Healthcare)\n",
        "\n",
        "- BERT captures the **nuances of medical language** (especially with domain-specific models like `BioBERT`, `ClinicalBERT`).\n",
        "- Understands **negations, context switches, and rare terminology** better than traditional methods.\n",
        "- Useful when summarizing **complex clinical notes or discharge summaries**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Summary Flow\n",
        "\n",
        "- üß© Break into sentences  \n",
        "- ü§ñ Encode each with BERT  \n",
        "- üìà Score importance  \n",
        "- ‚úÇÔ∏è Select top N as summary"
      ],
      "metadata": {
        "_uuid": "6f6657e5-a61d-4a88-91f9-6203acef170c",
        "_cell_guid": "4f53007d-b61f-4aa8-bca7-1f9049fb465d",
        "trusted": true,
        "collapsed": false,
        "execution": {
          "iopub.status.busy": "2025-04-14T08:31:06.904145Z",
          "iopub.execute_input": "2025-04-14T08:31:06.904901Z",
          "iopub.status.idle": "2025-04-14T08:31:06.908186Z",
          "shell.execute_reply.started": "2025-04-14T08:31:06.904878Z",
          "shell.execute_reply": "2025-04-14T08:31:06.907385Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "XbfWS9e9l9OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-extractive-summarizer --quiet"
      ],
      "metadata": {
        "_uuid": "a51f3869-b5d9-4eb2-954d-4fceb08e211c",
        "_cell_guid": "b3d59955-d2ca-4f2d-a699-0e6f9061a088",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "nWqYT7sXl9OJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "text = df.iloc[idx]['transcription']\n",
        "print(text)"
      ],
      "metadata": {
        "_uuid": "501e6524-bd6f-4932-b521-c67e3cc07459",
        "_cell_guid": "4efc8cf2-f8ba-4a17-b8fd-ca36feff315a",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "BDrKMVMll9OJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer import Summarizer\n",
        "from transformers import AutoTokenizer, AutoModel, logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "d_tokenizer = AutoTokenizer.from_pretrained('medical-ner-proj/bert-medical-ner-proj')\n",
        "d_model = AutoModel.from_pretrained('medical-ner-proj/bert-medical-ner-proj', output_hidden_states=True)\n",
        "\n",
        "model = Summarizer(custom_model=d_model, custom_tokenizer=d_tokenizer)\n",
        "result = model(text, ratio=0.2)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "_uuid": "03bf00cb-32e2-4ec6-a2e8-10f3b5da7a80",
        "_cell_guid": "ac1dd410-0043-4b9f-8cb4-d20c8cbe50f4",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6ikiZ-bxl9OJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Abstractive Summarization\n",
        "\n",
        "**Definition:**  \n",
        "Abstractive summarization generates new sentences that paraphrase and compress the original content ‚Äî similar to how a human might write a summary.\n",
        "\n",
        "**Strengths:**\n",
        "- More concise and coherent summaries.\n",
        "- Capable of paraphrasing, rewording, and synthesizing information.\n",
        "- Suitable for complex clinical narratives and reports.\n",
        "\n",
        "**Weaknesses:**\n",
        "- Requires more computational resources and training data.\n",
        "- Higher risk of introducing factual inaccuracies (\"hallucinations\").\n",
        "- May require fine-tuning for domain-specific tasks like healthcare.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "_uuid": "6761cd77-3a18-4f2a-81d3-58b6a5e4dc87",
        "_cell_guid": "9db68937-7baa-4775-a7f1-3dfa2a42bd0a",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "i-YIs093l9OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1: Huggingface Transformer with pipeline\n",
        "### üß† Intuition Behind Abstractive Summarization using Hugging Face Transformers (with pipeline)\n",
        "In modern NLP development, researchers and developers often seek high-level abstractions that eliminate the need to manage tokenization, attention mechanisms, and sequence generation manually. Hugging Face‚Äôs transformers library makes this possible with its powerful and elegant pipeline API.\n",
        "\n",
        "> üó£Ô∏è **The pipeline handles all the complexity ‚Äî so you can focus on exploring what matters most: the text.\"**\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ What is the Hugging Face pipeline?\n",
        "The pipeline is a high-level API designed to perform ready-to-use NLP tasks, such as:\n",
        "\n",
        "- Summarization (Abstractive)\n",
        "\n",
        "- Text classification\n",
        "\n",
        "- NER\n",
        "\n",
        "- Translation\n",
        "\n",
        "- Question answering\n",
        "\n",
        "All while hiding the underlying complexity of tokenizers, models, and pre/post-processing.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How the Summarization Pipeline Works (Behind the Scenes)\n",
        "1. **Input: Raw text (e.g., a clinical note or patient report).**\n",
        "\n",
        "2. **Preprocessing: Automatic tokenization using a model-specific tokenizer.**\n",
        "\n",
        "3. **Model Inference: Uses a transformer model to generate a new sequence of words ‚Äî not just copied from the original.**\n",
        "\n",
        "4. **Postprocessing: Converts tokens back to human-readable summary.**\n",
        "\n",
        "The process is abstractive, meaning it rephrases and compresses the content using deep learning, just like a human might summarize it.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why Use the Hugging Face pipeline?\n",
        "- ‚ö° Fast Setup: Get started with just 2 lines of code.\n",
        "\n",
        "- üß† State-of-the-Art Models: Use advanced models like facebook/bart-large-cnn, t5-small, google/pegasus-xsum, etc.\n",
        "\n",
        "- üîÅ Domain Adaptability: Swap models to suit your domain (e.g., use biomedical-trained models for healthcare).\n",
        "\n",
        "- üß™ No Engineering Overhead: Perfect for testing ideas without setting up custom architecture or training pipelines.\n",
        "\n",
        "- üß¨ Especially Useful in Healthcare NLP\n",
        "- üè• Can summarize long patient records, discharge notes, or radiology reports.\n",
        "\n",
        "- ‚è±Ô∏è Helps physicians quickly grasp patient history or trends.\n",
        "\n",
        "- üîÑ Can be integrated into clinical decision support tools with minimal code."
      ],
      "metadata": {
        "_uuid": "06b5acfb-d3dd-421d-866f-64dcdee8f939",
        "_cell_guid": "413b0476-604c-418b-9d44-c4fa88523ed8",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3yCCRwuRl9OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "text = df.iloc[idx]['transcription']\n",
        "print(text)"
      ],
      "metadata": {
        "_uuid": "c2308911-9f01-4e38-bb4f-b7a9f7666d82",
        "_cell_guid": "d52df77b-90c4-4413-8e90-3aeeb0af6f16",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8e8LI3psl9OK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model = \"umeshramya/t5_small_medical_512\"\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model)\n",
        "summary = summarizer(text, max_length=100)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "_uuid": "6da1cd0c-b0b1-4db7-ae29-0df2d534c8b6",
        "_cell_guid": "ac6e6e28-1819-4ab9-9c39-b317ff08d7a5",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2Ealherhl9OK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Intuition Behind Summarization using **LLM via API**\n",
        "\n",
        "With the rise of large language models (LLMs) like **GPT-4**, **LLaMA 3**, and **Mixtral**, summarization has become **more fluent, contextual, and human-like** than ever before. Rather than building and hosting these large models locally (which demands huge compute power), many developers and researchers now access them via **APIs**.\n",
        "\n",
        "> üó£Ô∏è **‚ÄúAPIs let you tap into the power of state-of-the-art LLMs ‚Äî without needing to train or run them yourself.‚Äù**\n",
        "\n",
        "---\n",
        "\n",
        "### üîå What Do We Mean by \"LLM via API\"?\n",
        "\n",
        "Using an LLM via API means sending your input (e.g., a clinical note) to a cloud-hosted model and receiving a summarized output in return. Common platforms include:\n",
        "\n",
        "- üß† **OpenAI (GPT series)**\n",
        "- üöÄ **Groq (LLaMA 3, Mixtral, Gemma)**\n",
        "- üî¨ **Cohere, Anthropic (Claude), Google (Gemini)**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How It Works\n",
        "\n",
        "1. **Prepare Your Input**  \n",
        "   Clinical or healthcare text that needs to be summarized.\n",
        "\n",
        "2. **Send to API**  \n",
        "   A POST request is made to the API with the model name, message history, and desired summary length.\n",
        "\n",
        "3. **Receive Summary**  \n",
        "   The model generates an abstractive, context-aware summary and returns it.\n",
        "\n",
        "4. **Display or Process Further**  \n",
        "   You can now present this summary in reports, dashboards, or alerts.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why Use LLMs via API?\n",
        "\n",
        "- üß† **High-quality abstractive summaries** (beyond simple sentence extraction)\n",
        "- ‚ö° **No need to maintain infrastructure**\n",
        "- üß™ **Great for rapid prototyping and experimentation**\n",
        "- üåç **Access to multilingual, domain-adapted models**\n",
        "- üíâ **Ideal for sensitive use-cases like healthcare**, where fluency and correctness are crucial\n",
        "\n",
        "---\n",
        "\n",
        "### üìâ Limitations\n",
        "\n",
        "- üîí **Data privacy concerns** (important in clinical environments)\n",
        "- üí∞ **Pay-per-request pricing models**\n",
        "- üåê **Requires internet and stable access**\n",
        "- ‚è≥ **Latency** may be higher than local models for real-time applications"
      ],
      "metadata": {
        "_uuid": "3202bbd2-b1fd-4a28-bba6-b4dffd16a634",
        "_cell_guid": "6d1a7a14-ad28-4a84-8995-ed7820f2e194",
        "trusted": true,
        "collapsed": false,
        "execution": {
          "iopub.status.busy": "2025-04-15T01:46:43.244285Z",
          "iopub.execute_input": "2025-04-15T01:46:43.244884Z",
          "iopub.status.idle": "2025-04-15T01:46:43.254279Z",
          "shell.execute_reply.started": "2025-04-15T01:46:43.244859Z",
          "shell.execute_reply": "2025-04-15T01:46:43.253334Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "m9iB6Z0Tl9OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"color:red;\">Before you start you need an API key</h3>\n",
        "\n",
        "1. In a new tab, open <a href=\"https://console.groq.com/login\">GROQ Cloud</a>\n",
        "2. Enter your email address and login. Please ensure you have access to that email's mailbox right now.\n",
        "3. Check your email and press the continue button in email you have received from GROQ Cloud.\n",
        "4. Press the \"Create API Key\" button.\n",
        "5. Copy the key and paste it in the cell below. Be sure to enclose the API key in quotation marks."
      ],
      "metadata": {
        "_uuid": "14b0d9cf-091f-4453-ac03-1e7eb95be779",
        "_cell_guid": "a0be692e-035a-4bbf-a48c-0d9b41b1ff8b",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b8RVSdEMl9OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"Enter your API Key here\""
      ],
      "metadata": {
        "_uuid": "955d2ca5-f856-4f1a-bbb0-3875e56668ef",
        "_cell_guid": "09599b44-083a-4799-b195-fd7d86f42f3e",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "f-aL0goEl9OL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "text = df.iloc[idx]['transcription']\n",
        "print(text)"
      ],
      "metadata": {
        "_uuid": "d5146b2c-f06d-4190-b8c0-a468bb1d1435",
        "_cell_guid": "8f583e22-aebd-4f24-ada5-4983c1c9d4c2",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wMWu-7WWl9OL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def groq_summarizer(text, max_length=200, model=\"llama3-8b-8192\", api_key=\"your_groq_api_key\"):\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful medical summarizer.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize the following clinical note under {max_length} words:\\n\\n{text}\"}\n",
        "        ],\n",
        "        \"temperature\": 0.5,\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "summary = groq_summarizer(text, max_length=100, api_key=API_KEY)\n",
        "print(summary)"
      ],
      "metadata": {
        "_uuid": "f716a409-0e17-46db-a527-2e157c67ea45",
        "_cell_guid": "81e23ac3-b884-485b-8a85-535cc115f46e",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "MdcpezyUl9OL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Structured Summarization\n",
        "\n",
        "Large Language Models (LLMs) accessed through APIs (like Groq, OpenAI, etc.) enable developers and researchers to **generate structured and clinically relevant summaries** from raw medical text without building domain-specific pipelines from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ What Is Structured Summarization?\n",
        "\n",
        "Unlike generic summaries, **structured summarization** formats the output into predefined sections ‚Äî for example:\n",
        "\n",
        "- **History**\n",
        "- **Past Medications**\n",
        "- **Current Problem**\n",
        "- **Plan**\n",
        "\n",
        "This mirrors the format that clinicians use during SOAP note-taking (Subjective, Objective, Assessment, Plan), making it more interpretable and actionable in real-world settings.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How It Works\n",
        "\n",
        "1. **Input**: A raw clinical note is passed to the LLM via an API call.\n",
        "2. **System Prompt**: You instruct the model to extract key information into predefined sections.\n",
        "3. **Model Inference**: The LLM parses the text and identifies relevant sentences based on the prompt.\n",
        "4. **Output**: A well-formatted summary with labeled clinical categories is returned.\n",
        "\n",
        "---\n",
        "\n",
        "> üó£Ô∏è **\"Instead of just shrinking the text, you're teaching the model how to think like a doctor taking notes.\"**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why Use LLM APIs for This?\n",
        "\n",
        "- üìã **Template-Aware Summaries**: Makes the information easier to review for both clinicians and researchers.\n",
        "- üß† **Zero-Shot Understanding**: No training required ‚Äî LLMs can generalize from just the prompt.\n",
        "- üïí **Rapid Prototyping**: You don‚Äôt need to train custom models or create clinical ontologies.\n",
        "- üîç **Domain-Agnostic Adaptation**: The same technique can be adapted to oncology, cardiology, psychiatry, etc."
      ],
      "metadata": {
        "_uuid": "fa3d342c-37b2-43f5-b1ed-d16bfdb3b5aa",
        "_cell_guid": "cc07eeb9-6bf9-4c49-a0de-738a1df6e228",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Ul2iN4LJl9OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 4\n",
        "text = df.iloc[idx]['transcription']\n",
        "text"
      ],
      "metadata": {
        "_uuid": "4e94c7d7-2838-4b01-ba15-2fe35a1e0083",
        "_cell_guid": "1e2fa316-a262-45f2-859c-e79ac2345a6e",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "hCgGGzTil9OM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def structured_clinical_summarizer(text, model=\"llama3-8b-8192\", api_key=\"your_groq_api_key\"):\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Given the clinical note below, extract a structured 1-liner summary with the following sections:\n",
        "    - History\n",
        "    - Past Medications\n",
        "    - Current Problem\n",
        "    - Plan\n",
        "\n",
        "    Present the output clearly labeled under each section. If the information is not present, leave it blank.\n",
        "\n",
        "    Clinical Note:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes medical notes into structured sections.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.3,\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
        "\n",
        "print(structured_clinical_summarizer(text,api_key=API_KEY))"
      ],
      "metadata": {
        "_uuid": "4ddf7255-eb2b-4c02-a624-ac52d2b0c35a",
        "_cell_guid": "cd6d1c47-f697-4a61-a39f-b8b377b81909",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "g4Obaj9ql9OM"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}