{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 482,
          "sourceType": "datasetVersion",
          "datasetId": 228
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "AI Health Workshop - Explainability",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Why Explainability Matters in Healthcare\n",
        "\n",
        "In healthcare, decisions made by AI models can have life-altering consequences. Unlike other domains, **trust, transparency, and accountability** are not just nice-to-haves‚Äîthey are essential.\n",
        "\n",
        "Let‚Äôs consider a simple example:\n",
        "\n",
        "---\n",
        "\n",
        "### üè• Example: Predicting Risk of Sepsis\n",
        "\n",
        "A hospital uses an AI model to predict whether a patient is at risk of developing **sepsis** based on electronic health records (EHR). The model flags a patient as \"high risk\", and the medical team responds by initiating aggressive treatment, including antibiotics and ICU monitoring.\n",
        "\n",
        "‚úÖ **The prediction is correct** ‚Äî the patient survives.  \n",
        "‚ùå **But the doctors have no idea why** the model made the prediction.\n",
        "\n",
        "Now consider the opposite:\n",
        "\n",
        "- The model **fails to flag** a patient who later develops sepsis.\n",
        "- The doctors, trusting the AI, delay intervention.\n",
        "- The outcome is tragic.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è What‚Äôs the problem?\n",
        "\n",
        "In both scenarios, the **\"black-box\" nature** of the AI model creates serious risks:\n",
        "\n",
        "- **Lack of transparency**: Clinicians can't verify or question the model's decision.\n",
        "- **No accountability**: It‚Äôs hard to trace errors back to specific reasoning.\n",
        "- **Trust deficit**: Clinicians are less likely to adopt or rely on AI tools they don't understand.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Explainability to the rescue\n",
        "\n",
        "By making AI decisions **understandable and interpretable**, we can:\n",
        "\n",
        "- Help clinicians **validate** model outputs against medical knowledge.\n",
        "- Enable **error analysis** when things go wrong.\n",
        "- Improve **regulatory acceptance** and **patient safety**.\n",
        "- Build **trust** between humans and machines.\n",
        "\n",
        "In short, explainability isn't just a technical feature‚Äîit's a **clinical necessity**.\n"
      ],
      "metadata": {
        "id": "R5Ij3Z79Snvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Steps: Loading a dataset"
      ],
      "metadata": {
        "id": "o2OhwckLSnvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "id": "3yRJb6xVSnvi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\"\n",
        "diabetes_df = pd.read_csv(url)\n",
        "diabetes_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "2eXj-FXoSnvi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(diabetes_df.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ahsN4vHVSnvj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Types of Explainability: Global vs Local\n",
        "\n",
        "When we talk about making AI models explainable, we need to consider **what kind of understanding** we are aiming for. There are two main types:\n",
        "\n",
        "- **Global Explanations**\n",
        "- **Local Explanations**\n",
        "\n",
        "![explanation-type.png](https://github.com/adilsal33m/ai-health-workshop/blob/main/explanation-type.png?raw=true)\n",
        "\n",
        "\n",
        "### üß© Putting it together\n",
        "\n",
        "| Type        | Focus                | Use Case Example           |\n",
        "|-------------|----------------------|-----------------------------|\n",
        "| Global      | Whole model behavior | Understanding key risk factors for disease across all patients |\n",
        "| Local       | Single prediction    | Explaining why a specific patient was flagged |\n",
        "\n",
        "Both perspectives are important‚Äîglobal helps us **trust the model**, while local helps us **trust individual predictions**.\n"
      ],
      "metadata": {
        "id": "K7qMWcrFSnvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåç Global Explainability using Logistic Regression\n",
        "\n",
        "Global explainability refers to **understanding the model as a whole**.\n",
        "\n",
        "It answers questions like:\n",
        "\n",
        "- _‚ÄúWhat features are generally important for this model?‚Äù_  \n",
        "- _‚ÄúHow does the model make decisions overall?‚Äù_  \n",
        "- _‚ÄúWhat are the high-level rules or patterns?‚Äù_\n",
        "\n",
        "### ü©∫ Example in Healthcare:\n",
        "\n",
        "In a model that predicts the risk of heart disease:\n",
        "> Global explainability might show that **age**, **cholesterol level**, and **smoking status** are the most important features overall.\n",
        "\n",
        "üßæ This kind of insight is useful for:\n",
        "- Auditing the model for fairness or bias.\n",
        "- Communicating with regulators and stakeholders.\n",
        "- Validating the model against domain knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Visualization\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/logistic-regression-global.png?raw=true\" width=300 style=\" margin: 0 auto;\">\n"
      ],
      "metadata": {
        "id": "EZl-bYdxSnvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "# Features and target\n",
        "X = diabetes_df.drop(columns=[\"Outcome\"])\n",
        "y = diabetes_df[\"Outcome\"]\n",
        "\n",
        "# Split the dataset for training and testing the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Standardize the features e.g. 3.5 -> 0.7\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predictions for unseen test records\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YILQTuIrSnvk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance\n",
        "feature_names = X.columns\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame for easy plotting\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Coefficient\": coefficients,\n",
        "}).sort_values(by=\"Coefficient\", ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=importance_df, x=\"Coefficient\", y=\"Feature\", palette=\"viridis\")\n",
        "plt.title(\"Global Feature Importance (Logistic Regression)\")\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7yfum3KmSnvk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Local Explainability using Logistic Regression\n",
        "\n",
        "Local explainability refers to **understanding why the model made a specific prediction for an individual case**.\n",
        "\n",
        "It answers questions like:\n",
        "\n",
        "- _‚ÄúWhy was this patient classified as high-risk?‚Äù_  \n",
        "- _‚ÄúWhat factors influenced this particular prediction?‚Äù_\n",
        "\n",
        "### ü©∫ Example in Healthcare:\n",
        "\n",
        "For a specific patient flagged at high risk for diabetes:\n",
        "> Local explainability might show that **high BMI**, **family history**, and **elevated glucose levels** were the main drivers.\n",
        "\n",
        "üßæ This is crucial for:\n",
        "- Clinical decision-making.\n",
        "- Gaining trust from doctors and patients.\n",
        "- Identifying and correcting individual misclassifications.\n",
        "\n",
        "### Visualization\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/logistic-regression-local.png?raw=true\" width=400 style=\"margin: 0 auto;\"/>"
      ],
      "metadata": {
        "id": "RBEv6YAgSnvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a single test instance\n",
        "idx = 1\n",
        "instance = X_test.iloc[idx]\n",
        "instance_scaled = X_test_scaled[idx]\n",
        "actual_label = y_test.iloc[idx]\n",
        "\n",
        "# Get prediction probability and class\n",
        "proba = model.predict_proba([instance_scaled])[0][1]\n",
        "pred_class = model.predict([instance_scaled])[0]\n",
        "\n",
        "print(f\"üìå Actual Label: {actual_label}\")\n",
        "print(f\"üß† Predicted Class: {pred_class}\")\n",
        "print(f\"üìà Probability of Diabetes: {proba:.3f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "8UTax7uhSnvl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute individual contributions in the final output\n",
        "contributions = instance_scaled * model.coef_[0]\n",
        "\n",
        "# Combine into a DataFrame\n",
        "local_df = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Value\": instance.values,\n",
        "    \"Scaled Value\": instance_scaled,\n",
        "    \"Coefficient\": model.coef_[0],\n",
        "    \"Contribution in the Final Output\": contributions\n",
        "}).sort_values(by=\"Contribution in the Final Output\", key=abs, ascending=False)\n",
        "\n",
        "# Display top contributing features\n",
        "local_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "mZojJR8_Snvl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize contributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=local_df, x=\"Contribution in the Final Output\", y=\"Feature\", palette=\"coolwarm\")\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.title(f\"Local Explanation for Test Instance #{idx}\")\n",
        "plt.xlabel(\"Contribution in the Final Output\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "TBQNHp-LSnvl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≥ Explainability with Decision Tree\n",
        "\n",
        "Decision Trees are inherently interpretable because they make predictions by following a **path of logical rules** (e.g., `Glucose > 130`, `BMI < 30`, etc.).\n",
        "\n",
        "Let‚Äôs train a Decision Tree and visualize the **decision path** for an individual patient.\n",
        "\n",
        "> Unlike Logistic Regression, Decision Trees don‚Äôt use additive contributions. Instead, the explanation is a series of **\"if this, then that\"** rules leading to the outcome.\n",
        "\n",
        "### Visualization\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/decision-tree-local.png?raw=true\" width=300 style=\"width: 300px; margin: 0 auto;\"/>\n"
      ],
      "metadata": {
        "id": "bnZtPyEdSnvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "# Features and target\n",
        "X = diabetes_df.drop(columns=[\"Outcome\"])\n",
        "y = diabetes_df[\"Outcome\"]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Train a Decision Tree\n",
        "tree_model = DecisionTreeClassifier(max_depth=3,random_state=42)  # keep it small for easy interpretation\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions on unseen test records\n",
        "y_pred = tree_model.predict(X_test)\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZbRm1pIrSnvm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict for the same instance\n",
        "idx = 0\n",
        "instance = X_test.iloc[idx]\n",
        "true_label = y_test.iloc[idx]\n",
        "predicted = tree_model.predict([instance])[0]\n",
        "proba = tree_model.predict_proba([instance])[0][1]\n",
        "\n",
        "print(f\"üìå Actual Label: {true_label}\")\n",
        "print(f\"üß† Predicted Class: {predicted}\")\n",
        "print(f\"üìà Probability of Diabetes: {proba:.3f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "fGE908fjSnvm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the decision path\n",
        "node_indicator = tree_model.decision_path([instance])\n",
        "feature_names = X.columns\n",
        "\n",
        "# Print conditions along the path\n",
        "leaf_id = tree_model.apply([instance])[0]\n",
        "feature_index = tree_model.tree_.feature\n",
        "threshold = tree_model.tree_.threshold\n",
        "\n",
        "print(\"üß© Decision Path:\")\n",
        "\n",
        "for node_id in node_indicator.indices:\n",
        "    if leaf_id == node_id:\n",
        "        continue\n",
        "\n",
        "    f_idx = feature_index[node_id]\n",
        "    thresh = threshold[node_id]\n",
        "    f_val = instance[feature_names[f_idx]]\n",
        "\n",
        "    if f_val <= thresh:\n",
        "        decision = f\"{feature_names[f_idx]} ‚â§ {thresh:.2f}\"\n",
        "    else:\n",
        "        decision = f\"{feature_names[f_idx]} > {thresh:.2f}\"\n",
        "\n",
        "    print(f\"‚Ü≥ {decision} (actual value: {f_val})\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "QuEPMJ7wSnvm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supertree --quiet"
      ],
      "metadata": {
        "trusted": true,
        "id": "3NxT09G-Snvm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from supertree import SuperTree\n",
        "super_tree = SuperTree(tree_model, X, y, list(X.columns), [\"No Diabetes\", \"Diabetes\"])\n",
        "\n",
        "# show tree in your notebook\n",
        "super_tree.show_tree()"
      ],
      "metadata": {
        "trusted": true,
        "id": "EoN_9euJSnvm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Interpretable Models vs Post-hoc Explainability Tools\n",
        "\n",
        "In the context of explainable AI (XAI), it's important to distinguish between **inherently interpretable models** and **post-hoc explainability tools**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Interpretable Models\n",
        "\n",
        "These models are **transparent by design** ‚Äî their internal mechanics can be easily understood by humans. Their decision-making process is **intrinsically explainable**.\n",
        "\n",
        "**Examples:**\n",
        "- Logistic Regression\n",
        "- Decision Trees\n",
        "\n",
        "**Benefits:**\n",
        "- Simpler to audit and validate\n",
        "- Easy to communicate to domain experts (e.g., clinicians)\n",
        "- No need for additional tools to understand behavior\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì Why Not Always Use Interpretable Models?\n",
        "\n",
        "While interpretable models are ideal for transparency, they often come with limitations:\n",
        "- May **lack predictive power** on complex datasets\n",
        "- Struggle to capture **non-linear relationships**\n",
        "- Performance trade-offs can be significant in real-world healthcare scenarios\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Post-hoc Explainability Tools\n",
        "\n",
        "Post-hoc methods are applied **after training a model** to help explain its predictions. These are especially useful when using **complex \"black-box\" models** like:\n",
        "- Random Forests\n",
        "- Gradient Boosted Trees\n",
        "- Deep Neural Networks\n",
        "\n",
        "**Examples of Post-hoc Tools:**\n",
        "- SHAP (SHapley Additive exPlanations)\n",
        "- LIME (Local Interpretable Model-agnostic Explanations)\n",
        "- CFE (Counterfactual Explanations)\n",
        "\n",
        "### Visualization\n",
        "\n",
        "![interpretable-vs-explainable.png](https://github.com/adilsal33m/ai-health-workshop/blob/main/interpretable-vs-explainable.png?raw=true)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-17T09:59:58.003205Z",
          "iopub.execute_input": "2025-04-17T09:59:58.003604Z",
          "iopub.status.idle": "2025-04-17T09:59:58.013149Z",
          "shell.execute_reply.started": "2025-04-17T09:59:58.003578Z",
          "shell.execute_reply": "2025-04-17T09:59:58.011914Z"
        },
        "id": "Hrwqp8MaSnvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Intuition Behind LIME (Local Interpretable Model-Agnostic Explanations)\n",
        "\n",
        "Imagine asking a complex medical expert why they made a certain diagnosis. They may have considered dozens of factors and years of experience. It's not easy to get a clear, simple answer.\n",
        "\n",
        "Now imagine asking:  \n",
        "üëâ _\"What were the **top 3 things** that made you think this patient has diabetes?\"_\n",
        "\n",
        "This is what **LIME** tries to do ‚Äî but with machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç How It Works (Intuition Only):\n",
        "\n",
        "Even though a model like Random Forest or XGBoost is too complex to understand all at once, we **only care about one prediction** at a time (like a single patient's result).\n",
        "\n",
        "So, LIME says:\n",
        "> \"Let's zoom in around this one prediction.  \n",
        "> I'll create some **slightly modified versions** of the input and see how the model behaves.\"\n",
        "\n",
        "By watching how the model's output changes with small tweaks to the input, LIME figures out:\n",
        "- Which features (like glucose level, BMI, age) are most influential for **this particular prediction**.\n",
        "- Whether each feature pushed the model toward predicting \"diabetes\" or \"no diabetes\".\n",
        "\n",
        "### Visualization\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/LIME.png?raw=true\" width=300 style=\" margin: 0 auto;\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "AZtnbSarSnvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime --quiet"
      ],
      "metadata": {
        "trusted": true,
        "id": "31InHewYSnvn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import xgboost as xgb\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Features and target\n",
        "X = diabetes_df.drop(columns=[\"Outcome\"])\n",
        "y = diabetes_df[\"Outcome\"]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=10, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# 4. Set up the LIME explainer\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=X_train.values,\n",
        "    feature_names=X.columns.tolist(),\n",
        "    class_names=['No Diabetes', 'Diabetes'],\n",
        "    mode='classification'\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "SXg5WE1QSnvn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a test instance to explain\n",
        "i = 0\n",
        "sample = X_test.iloc[i].values\n",
        "true_label = y_test.iloc[i]\n",
        "print(\"True Label:\", true_label)\n",
        "print(\"Predicted Label:\", xgb_model.predict([sample])[0])\n",
        "\n",
        "# 7. Generate explanation\n",
        "exp = explainer.explain_instance(sample, xgb_model.predict_proba, num_features=8)\n",
        "\n",
        "# Get explanation as a list of (feature, weight) tuples\n",
        "weights = exp.as_list()\n",
        "features, contributions = zip(*weights)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "colors = ['red' if w > 0 else 'green' for w in contributions]\n",
        "plt.barh(features, contributions, color=colors)\n",
        "plt.axvline(0, color='black', linewidth=0.8)\n",
        "plt.xlabel(\"Contribution to prediction\")\n",
        "plt.title(\"LIME Explanation (Static View)\")\n",
        "plt.gca().invert_yaxis()  # Highest contribution on top\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Knc_xsKWSnvn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Intuition Behind SHAP (SHapley Additive exPlanations)\n",
        "\n",
        "Imagine you want to understand why a **doctor** diagnosed a patient with **diabetes**. The doctor considered multiple factors like age, BMI, blood sugar level, etc.\n",
        "\n",
        "You could ask the doctor:\n",
        "> _\"What role did each factor play in the final diagnosis?\"_\n",
        "\n",
        "**SHAP** answers this question, but in a more precise and mathematical way. It takes each feature (age, BMI, blood sugar, etc.) and tells you **how much that feature contributed** to the final decision ‚Äî in terms of **shifting** the prediction.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç How It Works (Intuition Only):\n",
        "\n",
        "Think of **SHAP** as a **teamwork approach**:\n",
        "- Imagine each feature (like blood sugar or BMI) as a team member.\n",
        "- The **SHAP value** of a feature is like saying, _\"How much did this feature **help** the team make the final prediction?\"_\n",
        "\n",
        "It uses **game theory** to measure each feature‚Äôs fair share of the prediction outcome, ensuring that:\n",
        "- All features are treated fairly.\n",
        "- The total contribution of all features adds up to the final prediction.\n",
        "\n",
        "**Example:** If we predict that a patient has diabetes:\n",
        "- **Blood sugar level** might contribute strongly (positive SHAP value) towards the diagnosis of diabetes.\n",
        "- **Age** might slightly **lower** the chance of diabetes (negative SHAP value).\n",
        "- **BMI** might have little effect, leading to a small SHAP value.\n",
        "\n",
        "### Visualization\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/SHAP.png?raw=true\" width=300 style=\"width: 300px; margin: 0 auto;\"/>"
      ],
      "metadata": {
        "id": "4WhScasrSnvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap --quiet"
      ],
      "metadata": {
        "trusted": true,
        "id": "TDrHZT4cSnvo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Features and target\n",
        "X = diabetes_df.drop(columns=[\"Outcome\"])\n",
        "y = diabetes_df[\"Outcome\"]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf_model.predict(X_test)\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Initialize SHAP explainer for Random Forest\n",
        "explainer = shap.TreeExplainer(rf_model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4AB_9aVhSnvo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a test instance to explain\n",
        "i = 4\n",
        "instance = X_test.iloc[i:i+1]\n",
        "print(instance,end=\"\\n\\n\")\n",
        "print(\"Prediction:\",\"Diabetes\" if rf_model.predict(instance)[0] == 1 else \"No Diabetes\")\n",
        "\n",
        "# Compute SHAP values for the instance\n",
        "shap_values = explainer(instance)\n",
        "shap.initjs()  # Initializes JS for SHAP plots\n",
        "shap.plots.waterfall(shap_values[0][:,1])"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ya8MAPv4Snvo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Intuition Behind Counterfactual Explanations (CFE)\n",
        "\n",
        "Imagine a doctor tells a patient, ‚ÄúYou are at high risk of developing diabetes.‚Äù  \n",
        "Naturally, the next question is: **\"What can I change to reduce that risk?\"**\n",
        "\n",
        "This is where **Counterfactual Explanations (CFEs)** come in.\n",
        "\n",
        "---\n",
        "\n",
        "### üö¶ What is a Counterfactual?\n",
        "\n",
        "A counterfactual answers the question:\n",
        "\n",
        "> _\"What is the closest change to this input that would lead to a different (usually better) prediction?\"_\n",
        "\n",
        "For example:\n",
        "- ‚ÄúIf your glucose level were 110 instead of 130, the model would not have flagged diabetes risk.‚Äù\n",
        "- ‚ÄúIf your BMI were just slightly lower, you would have been predicted as low-risk.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ How It Works (Intuitively)\n",
        "\n",
        "1. Take the input data (e.g., patient metrics).\n",
        "2. Search for a new data point that:\n",
        "   - Is as **similar** as possible to the original.\n",
        "   - But leads to a **different prediction** (e.g., no diabetes).\n",
        "3. Present the **differences** as the explanation.\n",
        "\n",
        "### Visualization\n",
        "<img src=\"https://github.com/adilsal33m/ai-health-workshop/blob/main/CFE.png?raw=true\" width=300 style=\"width: 300px; margin: 0 auto;\"/>\n"
      ],
      "metadata": {
        "id": "C38RjCcXSnvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dice-ml --quiet"
      ],
      "metadata": {
        "trusted": true,
        "id": "nK2ov1a2Snvo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import dice_ml\n",
        "from dice_ml import Dice\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Features and target\n",
        "X = diabetes_df.drop(columns=[\"Outcome\"])\n",
        "y = diabetes_df[\"Outcome\"]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Standardize the features (important for NN training)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network model\n",
        "nn_model = keras.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='sigmoid'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(16, activation='sigmoid'),\n",
        "    layers.Dense(8, activation='sigmoid'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "nn_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "nn_model.fit(X_train_scaled, y_train, epochs=20, batch_size=16, verbose=0)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_probs = nn_model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Initialize DiCE explainer\n",
        "data_dice = dice_ml.Data(\n",
        "    dataframe=diabetes_df,\n",
        "    continuous_features=[col for col in diabetes_df.columns if col != \"Outcome\"],\n",
        "    outcome_name=\"Outcome\"\n",
        ")\n",
        "model_dice = dice_ml.Model(model=nn_model, backend=\"TF2\")\n",
        "exp = Dice(data_dice, model_dice)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gL23i0yjSnvp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Select an instance to generate counterfactual for (we'll select on positive instance)\n",
        "y_positive = np.where(rf_model.predict(X_test) == 1)[0]\n",
        "\n",
        "idx = y_positive[0]\n",
        "query_instance = X_test.iloc[idx:idx+1]  # Pick any index from test set\n",
        "\n",
        "print(\"Actual label:\", y_test.iloc[idx])\n",
        "print(\"Prediction:\", nn_model.predict(query_instance)[0])\n",
        "\n",
        "# Generate counterfactual explanation\n",
        "counterfactuals = exp.generate_counterfactuals(query_instance,\n",
        "                                               total_CFs=1,\n",
        "                                               desired_class=\"opposite\",\n",
        "                                               features_to_vary=['Glucose','BloodPressure','Insulin','BMI'],\n",
        "                                               diversity_weight=0.1\n",
        "                                              )\n",
        "\n",
        "# Visualize the counterfactual result\n",
        "counterfactuals.visualize_as_dataframe(show_only_changes=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nHXQ6D-LSnvp"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}